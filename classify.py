# -*- coding: utf-8 -*-
"""IA-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHavZ76wSCFwGKrdmPryYi-66ADRJt9t

> **URL Classification**

**Problem Statement**

Web page classification has become a challenging task due to the exponential growth of the World Wide Web. As a result, high accuracy may not be achievable as URL contains minimal information. Further, noisy and irrelevant features present in URL demand feature selection methods for URL classification. Therefore, a technique will be robust to irrelevant features and will generalize the classification for wide range of websites.

Imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from collections import Counter
from tqdm import tqdm
from random import random, choices, shuffle, randint
from keras import Sequential
from keras.layers import Dense

"""Mounting to Google Drive"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Changing the Directory"""

cd gdrive/"My Drive"/LY/DWM/IA2

"""Loading the Dataset"""

names = ["Category","Title","Description"]
dataset = pd.read_csv("dataset.csv",names=names, na_filter=False)[1:]

"""**Preprocessing**

Histogram of Categories
"""

dataset.Category.value_counts().plot(figsize=(12,5),kind='bar',color='green');
plt.xlabel('Category')
plt.ylabel('Total Number Of Individual Category for Training')

"""Removing all the unnecessary characters, converting the words to root form and converting them Vector form"""

#Getting input and output from the dataframe and converting them to list
X = dataset[names[1:]].values.tolist()
Y = dataset[names[0]].values.tolist()

X1 = dataset[names[1:]].values.tolist()
Y1 = dataset[names[0]].values.tolist()


#Storing all preprocessing data
corpus = []


for i in tqdm(range(len(dataset)), position=0, leave=True):

    #Removing all non-alphabetical characters
    review = re.sub('[^a-zA-Z]', ' ', X[i][0]+" "+X[i][1])

    #Converting to lower case
    review = review.lower()

    #Splitting the words
    review = review.split()

    #Converting all the words to their root form
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]

    #Joining the words again
    review = ' '.join(review)
    corpus.append(review)


#Vectorizing the input
cv_x = CountVectorizer(max_features=25000)
X = np.array(cv_x.fit_transform(corpus).toarray())
X_feature_names = cv_x.get_feature_names()

#Vectorizing the output
cv_y = CountVectorizer()
Y = np.array(cv_y.fit_transform(Y).toarray())
Y_feature_names = cv_y.get_feature_names()

"""Train-Test split"""

#80-20 Train Test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20)

"""**Training**

Initializing the model
"""

model=Sequential()
model.add(Dense(50,input_shape=(len(X_feature_names),),activation="relu"))
model.add(Dense(20,activation="relu"))
model.add(Dense(len(Y_feature_names),activation="softmax"))

"""Compiling and Training"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=5, batch_size=50)

"""**Testing**"""

prediction = model.predict(X_test)
y_test = np.argmax(Y_test,axis=1)
y_pred = np.argmax(prediction,axis=1)

"""Accuracy Check"""

accuracy = metrics.accuracy_score(y_test, y_pred)
print(accuracy*100)

"""Confusion Matrix"""

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
print(np.unique(dataset[names[0]].values.tolist()))
print(confusion_matrix)

"""**Demo**"""

n = randint(0,len(X)-1)
print("Input:-")
print("Title, Description")
print(X1[n])
prediction = np.argmax(model.predict(np.array([X[n]])), axis=1)[0]
print("")
print("Prediction:-")
print(Y_feature_names[prediction])
print("")
print("Actual:-")
print(Y1[n])

def get(X,X_feature_names):
  corpus = []



  #Removing all non-alphabetical characters
  review = re.sub('[^a-zA-Z]', ' ', X[0]+" "+X[1])
  #Converting to lower case
  review = review.lower()

  #Splitting the words
  review = review.split()

  #Converting all the words to their root form
  ps = PorterStemmer()
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]

  #Joining the words again
  review = ' '.join(review)
  corpus.append(review)
  corpus = list(corpus[0].split())

  X = [0 for i in range(len(X_feature_names))]
  for i in range(len(corpus)):
    if corpus[i] in X_feature_names:
      X[X_feature_names.index(corpus[i])]+=1
  

  
  return np.array([X])


print("Enter Input:-")
print("Title, Description")
title = input()
description = input()
prediction = np.argmax(model.predict(get([title,description],X_feature_names)), axis=1)[0]
print("")
print("Prediction:-")
print(Y_feature_names[prediction])